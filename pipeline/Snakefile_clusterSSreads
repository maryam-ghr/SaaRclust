from collections import defaultdict

configfile: "config.yaml"
SHORTREADS = config["shortreads"]
ALIGNERBINPATH = config["graphalignerfolder"] + "bin"
SCRIPTPATH = config["graphalignerfolder"] + "scripts"
BCALMPATH = config["bcalmpath"]
CONVERTTOGFAPATH = config["bcalm_converttoGFApath"]
BGREAT = config["bgreat"]
EXTRACTFASTQ = config["extract_fastq"]
inputDir = config["input_dir"]
softClustDir = config["soft_clust_dir"]
outputDir = config["output_dir"]
SSfastqDir = config["SS_fastq_dir"]
VG = config["VG"]
nodelens = config["node_lens"]
chroms = [str(x) for x in range(1, 23)] + ["X", "Y"]

wildcard_constraints:
	graphname = "k\d+_a\d+_u\d+",
	k = "\d+",
	a = "\d+",
	u = "\d+",
	longnodesize = "\d+",
	overlapsize = "\d+",
	longreads = "[^_]+",
	shortreads = "[^_]+",
	lib = "[^_]+_[0-9]+"


# outout dir should have these two files: cluster_name_mapping.data and clust_partners.txt

clusters = ["V" + str(i) for i in range(1, 30)] + ["V" + str(i) for i in range(31, 48)]

sample, chunkID, = glob_wildcards(inputDir+"/{sample}_chunk{chunkID}.maf.gz")
sample=set(sample)
libs, L, = glob_wildcards(SSfastqDir + "/{lib}_L{L}_R1_001.fastq.gz")


rule all:
	input:
		#expand(SSfastqDir + "/{fastqname}_{cluster}.fastq", fastqname = [libs[i] + "_L" + L[i] + "_R1_001" for i in range(len(libs))], cluster = clusters)
		#expand("graph_alignment/{fastqprefix}_R1_001_{cluster}_params_k{k}_a{a}_u{u}.data", fastqprefix=[libs[i] + "_L" + L[i] for i in range(len(libs))], cluster=clusters, k=config["k"], a = config["kmer_abundance"], u = config["unitig_abundance"]),
		#expand("graph_alignment/nodes_cluster{cluster}_params_k{k}_a{a}_u{u}.data", cluster=clusters, k=config["k"], a = config["kmer_abundance"], u = config["unitig_abundance"]),
		#expand("tmp/graph_k{k}_a{a}_u{u}.vg", k=config["k"], a = config["kmer_abundance"], u = config["unitig_abundance"]),
		#"graph_alignment/graph_nodes_inter_chr_intersection.RData",
		expand("mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.bam", k=config["k"], a = config["kmer_abundance"], u = config["unitig_abundance"]),
		expand("tmp/bubbles_k{k}_a{a}_u{u}.fa", k=config["k"], a = config["kmer_abundance"], u = config["unitig_abundance"])

rule add_soft_clust_to_original_map_files:
	input:
		minimap_file = inputDir+"/{sample}_chunk{chunkID}.maf.gz",
		soft_clust_file = softClustDir+"/{sample}_chunk{chunkID}_clusters.RData"
	output: outputDir+"/{sample}_chunk{chunkID}.maf"
	log: "log/add_soft_clust_{sample}_chunk{chunkID}.log"
	script: "utils/addSoftProbs.R"

rule output_header:
	input: expand(outputDir+"/{sample}_chunk{chunkID}.maf", sample=sample, chunkID=chunkID)
	output: outputDir + "/header.txt"
	log: "log/output_header.log"
	shell: "(time set +o pipefail && head -1 {input[0]} > {output})  > {log} 2>&1"

rule remove_slash_in_lib_names:
	input: outputDir+"/{sample}_chunk{chunkID}.maf",
	output: outputDir+"/{sample}_chunk{chunkID}.new.maf"
	log: "log/remove_slash_in_lib_names_{sample}_chunk{chunkID}.log"
	shell: "(time tail -n +2 {input} | awk '{{sub(/.{{1}}/, \"\", $3)}}1' > {output}) > {log} 2>&1"

rule split_chunk_by_lib:
	input:
		minimap_file = outputDir+"/{sample}_chunk{chunkID}.new.maf"
	output: [outputDir+"/splitted-per-lib/{sample}_chunk{chunkID}_" + x for x in expand("{lib}.maf", lib=libs)],
	log: "log/split_chunk_by_lib_{sample}_chunk{chunkID}"
	shell: "(time awk '{{print>\"{outputDir}/splitted-per-lib/{wildcards.sample}_chunk{wildcards.chunkID}_\"$3\".maf\"}}' {input.minimap_file} && touch {output}) > {log} 2>&1"

rule merge_files_with_same_lib:
	input: [outputDir + x + "{lib}.maf" for x in expand("/splitted-per-lib/{sample}_chunk{chunkID}_", sample=sample, chunkID=chunkID)]
	output: outputDir+"/splitted-per-lib/aln_{lib}.maf"
	log: "log/merge_files_with_same_lib_{lib}.log"
	shell: "(time cat {input} > {output}) > {log} 2>&1"

rule sort_lib_alignments:
	input:
		header = outputDir + "/header.txt",
		splittedChunks = [outputDir + x + "{lib}.maf" for x in expand("/splitted-per-lib/{sample}_chunk{chunkID}_", sample=sample, chunkID=chunkID)],
		libAlignment = outputDir+"/splitted-per-lib/aln_{lib}.maf"
	output: outputDir+"/splitted-per-lib/aln_{lib}_sorted.maf"
	log: "log/sort_lib_alignments_{lib}.log"
	shell: "(time cat {input.header} > {output} && sort -k1,1 {input.libAlignment} >> {output}) > {log} 2>&1"

rule cluster_SS_reads:
	input:
		aln_lib = outputDir+"/splitted-per-lib/aln_{lib}_sorted.maf",
		clust_to_chrom_mapping = outputDir + "/cluster_name_mapping.data",
		cluster_pairs = outputDir + "/clust_partners.txt"
	output: outputDir+"/splitted-per-lib/clust_{lib}.data"
	log: "log/cluster_SS_reads_{lib}.log"
	script: "utils/cluster_SS_reads.snakemake.R"

rule split_SS_read_names_by_clust:
	input: outputDir+"/splitted-per-lib/clust_{lib}.data"
	output: expand(outputDir+"/splitted-per-lib/clust_splitted_{{lib}}_{cluster}.data", cluster=clusters)
	log: "log/split_SS_read_names_by_clust_{lib}.log"
	shell: "(time awk -F, '{{if ($2 != \"\")print $1> \"{outputDir}/splitted-per-lib/clust_splitted_\" $3 \"_\" $2 \".data\"}}' {input}) > {log} 2>&1"

rule extract_fastq_per_lib_per_clust:
	input:
		SS_read_names = outputDir+"/splitted-per-lib/clust_splitted_{lib}_{cluster}.data",
		SS_fastq = SSfastqDir + "/{lib}_L{L}_R1_001.fastq.gz"
	output: SSfastqDir + "/{lib}_L{L}_R1_001_{cluster}.fastq"	
	log: "log/extract_fastq_per_lib_per_clust.log_{lib}_{L}_{cluster}"
	shell: "(time {EXTRACTFASTQ} {input} > {output}) > {log} 2>&1"

rule merge_fastq_files_with_same_clust:
	input: expand(SSfastqDir + "/{fastqprefix}_R1_001_{{cluster}}.fastq", fastqprefix=[libs[i] + "_L" + L[i] for i in range(len(libs))])
	output: SSfastqDir + "/all_SS_libs_cluster{cluster}.fastq"
	log: "log/merge_fastq_files_with_same_clust_{cluster}.log"
	shell: "(time cat {input} > {output}) > {log} 2>&1"


### De Bruijn graph building MIKKO
rule format_input_files:
	output:
		temp("filelist")
	shell:
		"printf '%s\\n' {SHORTREADS}/*.fastq.gz > {output}"

rule run_bcalm2:
	input:
		"filelist"
	output:
		temp("filelist_k{k}_a{a}.unitigs.fa")
	shadow: "shallow"
	threads: 8
	shell:
		"{BCALMPATH} -in {input} -out filelist_k{wildcards.k}_a{wildcards.a} -kmer-size {wildcards.k} -abundance-min {wildcards.a} -nb-cores {threads}"

rule filter_unitig_coverage:
	input:
		"filelist_k{k}_a{a}.unitigs.fa"
	output:
		temp("tmp/filelist_k{k}_a{a}_u{u}.unitigs.fa")
	shell:
		"python {SCRIPTPATH}/filter_bcalm_by_frequency.py {input} {wildcards.u} {output}"

rule convert_graph:
	input:
		"tmp/filelist_k{k}_a{a}_u{u}.unitigs.fa"
	output:
		temp("tmp/graph_k{k}_a{a}_u{u}_tipped.gfa")
	shell:
		"{CONVERTTOGFAPATH} {input} {output} {wildcards.k}"

rule untip_graph:
	input:
		"tmp/graph_k{k}_a{a}_u{u}_tipped.gfa"
	output:
		temp("tmp/graph_k{k}_a{a}_u{u}_untip.gfa")
	shell:
		"{ALIGNERBINPATH}/UntipRelative 1000 100 0.1 < {input} > {output}"

rule extract_biggest_component:
	input:
		"tmp/graph_k{k}_a{a}_u{u}_untip.gfa"
	output:
		temp("tmp/graph_k{k}_a{a}_u{u}_component.gfa")
	shell:
		"python {SCRIPTPATH}/extract_gfa_biggest_component.py {input} > {output}"

rule get_contigs:
	input:
		"tmp/graph_k{k}_a{a}_u{u}_component.gfa"
	output:
		temp("contigs_k{k}_a{a}_u{u}.fa")
	shell:
		"grep S < {input} | awk '{{print \">\" $NR; print $3;}}' > {output}"

rule remake_bcalm_from_contigs:
	input:
		"contigs_k{k}_a{a}_u{u}.fa"
	output:
		"contigs_k{k}_a{a}_u{u}.unitigs.fa"
	shadow: "shallow"
	threads: 8
	shell:
		"{BCALMPATH} -in {input} -out contigs_k{wildcards.k}_a{wildcards.a}_u{wildcards.u} -abundance-min 1 -kmer-size {wildcards.k} -nb-cores {threads}"

rule final_graph:
	input:
		"contigs_k{k}_a{a}_u{u}.unitigs.fa"
	output:
		"tmp/graph_k{k}_a{a}_u{u}.gfa"
	shell:
		"{CONVERTTOGFAPATH} {input} {output} {wildcards.k}"

rule convert_gfa_to_vg:
	input: "tmp/graph_k{k}_a{a}_u{u}.gfa"
	output: "tmp/graph_k{k}_a{a}_u{u}.vg"
	log: "log/convert_gfa_to_vg.log"
	shell: "{VG} view -v -F {input} > {output}"

rule align_SS_reads_to_DBG:
	input:
		SS_fastq = SSfastqDir + "/all_SS_libs_cluster{cluster}.fastq",
		graph = "contigs_k{k}_a{a}_u{u}.unitigs.fa"
	output: "graph_alignment/aln_cluster{cluster}_params_k{k}_a{a}_u{u}.data"
	threads: 8
	log: "log/align_SS_reads_to_DBG_cluster{cluster}_params_k{k}_a{a}_u{u}.log"
	shell: "(time {BGREAT} -t {threads} -k {wildcards.k} -u {input.SS_fastq} -g {input.graph} -q -a 31 -f {output}) > {log} 2>&1"


######################################
#finding heterozygous unitigs
######################################

## computing the ground truth het unitifs:
## mapping unitigs to the reference genome

rule index_ref:
	input: config["reference"]
	output:
		config["reference"] + ".amb",
		config["reference"] + ".ann",
		config["reference"] + ".bwt",
		config["reference"] + ".pac",
		config["reference"] + ".sa"
	log: "log/bwa_index.log"
	shell: "(time bwa index {input}) > {log} 2>&1"

rule bwa_map_unitigs_to_ref:
	input:
		ref=config["reference"],
		unitigs="contigs_k{k}_a{a}_u{u}.unitigs.fa"
	output: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.bam"
	log: "log/bwa_map_unitigs_to_ref.log"
	shell:
		"(time bwa mem -t 32 {input} | samtools view -Sb - > {output}) > {log} 2>&1"

rule sort_bam:
	input: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.bam",
	output: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam"
	log: "log/sort_bam.log"
	shell: "(time samtools sort -o {output} {input}) > {log} 2>&1"

rule index_sorted_bam:
	input: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam"
	output : "mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam.bai"
	log: "log/index_bam.log"
	shell: "(time set +o pipefail && samtools index {input}) > {log} 2>&1"

rule intersect_bam_vcf:
	input: 
		bam="mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam",
		vcf=[config["vcf_prefix"] + c + ".vcf" for c in chroms]
	output: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.bam"
	log: "log/intersect_bam_vcf.log"
	shell: "(time bedtools intersect -a {input.bam} -b {input.vcf} > {output}) > {log} 2>&1"

## bubble detection
rule bubble_detection:
	input: "tmp/graph_k{k}_a{a}_u{u}.gfa"
	output: "tmp/bubbles_k{k}_a{a}_u{u}.fa"
	log: "log/bubble_detection.log"
	shell: "(time python simple_bubbles.py {input} > {output}) > {log} 2>&1"	


######################################
rule output_unique_covered_nodes:
	input: "graph_alignment/aln_cluster{cluster}_params_k{k}_a{a}_u{u}.data"
	output: "graph_alignment/nodes_cluster{cluster}_params_k{k}_a{a}_u{u}.data"
	run:
		lines = open(input[0]).read()
		l = list(set(lines.replace(";", " ").split()))
		open(output[0], "w").write("\n".join(l))

rule find_inter_chr_intersection:
	input:
		nodes = expand("graph_alignment/nodes_cluster{cluster}_params_k{k}_a{a}_u{u}.data", cluster=clusters, k=config["k"], a = config["kmer_abundance"], u = config["unitig_abundance"]),
		cluster_pairs = outputDir + "/clust_partners.txt",
		node_lens = nodelens
	output:	"graph_alignment/graph_nodes_inter_chr_intersection.RData"
	log: "log/find_inter_chr_intersection.log"
	script: "utils/find_inter_chr_intersection.snakemake.R"
